{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oPAJl2uDmil"
      },
      "source": [
        "## Supervised fine-tuning (SFT) of an LLM\n",
        "\n",
        "Recall that creating a ChatGPT at home involves 3 steps: (see [Karpathy's State of GPT](https://youtu.be/bZQun8Y4L2A?si=Xei-QdscjdGMCcYP))\n",
        "\n",
        "1. pre-training a large language model (LLM) to predict the next token on internet-scale data, on clusters of thousands of GPUs. One calls the result a \"base model\"\n",
        "2. supervised fine-tuning (SFT) to turn the base model into a useful assistant\n",
        "3. human preference fine-tuning which increases the assistant's friendliness, helpfulness and safety.\n",
        "\n",
        "![sft image](./GPT_Assistant_training_pipeline_2.png)\n",
        "\n",
        "In this notebook, we're going to illustrate step 2. This involves supervised fine-tuning (SFT for short), also called instruction tuning.\n",
        "\n",
        "Supervised fine-tuning takes in a \"base model\" from step 1, i.e. a model that has been pre-trained on predicting the next token on internet text, and turns it into a \"chatbot\"/\"assistant\". This is done by fine-tuning the model on human instruction data, using the cross-entropy loss. This means that the model is still trained to predict the next token, although we now want the model to generate useful completions given an instruction like \"what are 10 things to do in London?\", \"How can I make pancakes?\" or \"Write me a poem about elephants\".\n",
        "\n",
        "To do this, one requires human annotators to collect useful completions, on which we can train the model. OpenAI for instance [hired human contractors for this](https://gizmodo.com/chatgpt-openai-ai-contractors-15-dollars-per-hour-1850415474), which were asked to generate useful completions given instructions, like \"In London, you can visit the Big Ben and (...)\". A nice collection of openly available SFT datasets can be found [here](https://huggingface.co/collections/HuggingFaceH4/awesome-sft-datasets-65788b571bf8e371c4e4241a).\n",
        "\n",
        "This way, the model becomes more useful: rather than simply predicting the next token (which might give undesirable outputs, like generating follow-up questions rather than answering the question), we now make it more likely that the model will output useful completions for any instruction we give it. We basically steer it in the direction of generating useful completions which a human could have written given any instruction.\n",
        "\n",
        "Notes:\n",
        "\n",
        "* the entire notebook is based on and can be seen as an annotated version of the [Alignment Handbook](https://github.com/huggingface/alignment-handbook) developed by Hugging Face, and more specifically the [recipe](https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/sft/config_lora.yaml) used to train Zephyr-7b-beta. Huge kudos to the team for creating this!\n",
        "* this notebook applies to any decoder-only LLM available in the Transformers library. In this notebook, we are going to fine-tune the [Mistral-7B base model](https://huggingface.co/mistralai/Mistral-7B-v0.1), which is one of the best open-source large language models at the time of writing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjdw05Rk5fYm"
      },
      "source": [
        "## Required hardware\n",
        "\n",
        "The notebook is designed to be run on any NVIDIA GPU which has the [Ampere architecture](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) or later with at least 24GB of RAM. This includes:\n",
        "\n",
        "* NVIDIA RTX 3090, 4090\n",
        "* NVIDIA A100, H100, H200\n",
        "\n",
        "and so on. Personally I'm running the notebook on an RTX 4090 with 24GB of RAM.\n",
        "\n",
        "The reason for an Ampere requirement is because we're going to use the [bfloat16 (bf16) format](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format), which is not supported on older architectures like Turing.\n",
        "\n",
        "But: a few tweaks can be made to train the model in float16 (fp16), which is supported by older GPUs like:\n",
        "\n",
        "* NVIDIA RTX 2080\n",
        "* NVIDIA Tesla T4\n",
        "* NVIDIA V100.\n",
        "\n",
        "Comments are added regarding where to swap bf16 with fp16.\n",
        "\n",
        "## Set-up environment\n",
        "\n",
        "Let's start by installing all the ü§ó goodies we need to do supervised fine-tuning. We're going to use\n",
        "\n",
        "* Transformers for the LLM which we're going to fine-tune\n",
        "* Datasets for loading a SFT dataset from the ü§ó hub, and preparing it for the model\n",
        "* BitsandBytes and PEFT for fine-tuning the model on consumer hardware, leveraging [Q-LoRa](https://huggingface.co/blog/4bit-transformers-bitsandbytes), a technique which drastically reduces the compute requirements for fine-tuning\n",
        "* TRL, a [library](https://huggingface.co/docs/trl/index) which includes useful Trainer classes for LLM fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9357hGFRqdi"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade transformers[torch] datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxpq51gW_ySc"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade bitsandbytes trl peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RgDwMbXpC4L"
      },
      "source": [
        "We also install [Flash Attention](https://github.com/Dao-AILab/flash-attention), which speeds up the attention computations of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNJvtR1wGxHm",
        "outputId": "545b75a3-932e-4195-f74a-a6f3447f67de"
      },
      "outputs": [],
      "source": [
        "# !pip install flash-attn --upgrade --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.5.1.post1'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import flash_attn\n",
        "flash_attn.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJIqlyI15kj8"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Note: the alignment handbook supports mixing several datasets, each with a certain portion of training examples. However, the Zephyr recipe only includes a single dataset, which is the [UltraChat200k dataset](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YhYvRDF25j59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# based on config\n",
        "raw_datasets = load_dataset(\"HuggingFaceH4/ultrachat_200k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFYvJUfODabt"
      },
      "source": [
        "The dataset contains various splits, each with a certain number of rows. In our case, as we're going to do supervised fine-tuning (SFT), only the \"train_sft\" and \"test_sft\" splits are relevant for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train_sft': 207865,\n",
              " 'test_sft': 23110,\n",
              " 'train_gen': 256032,\n",
              " 'test_gen': 28304}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the number of samples in each of the splits \n",
        "{k:raw_datasets[k].num_rows for k in raw_datasets.keys()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX1sIK6X6Opi",
        "outputId": "f0ab2b40-6789-44c4-ece2-bd84e0d5fa65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'messages'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['prompt', 'prompt_id', 'messages'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# remove this when done debugging\n",
        "indices = range(0,1000)\n",
        "\n",
        "dataset_dict = {\"train\": raw_datasets[\"train_sft\"].select(indices),\n",
        "                \"test\": raw_datasets[\"test_sft\"].select(indices)}\n",
        "\n",
        "raw_datasets = DatasetDict(dataset_dict)\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sByKH4hd8mUm"
      },
      "source": [
        "Let's check one example. The important thing is that each example should contain a list of messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2XwVpT17TAb",
        "outputId": "8d441fee-4a0b-4310-9a30-86b4439e0609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['prompt', 'prompt_id', 'messages'])\n"
          ]
        }
      ],
      "source": [
        "example = raw_datasets[\"train\"][0]\n",
        "print(example.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaWnT4uBCjTy"
      },
      "source": [
        "Each message is a dictionary containing 2 keys, namely:\n",
        "\n",
        "* \"role\": specifies who the creator of the message is (could be \"system\", \"assistant\" or \"user\" - the latter referring to a human).\n",
        "* \"content\": the actual content of the message.\n",
        "\n",
        "Let's print out the sequence of messages for this training example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ1sMda27Zj6",
        "outputId": "04b5ab19-2910-4f1b-91cb-3aece687e49b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user                :  These instructions apply to section-based themes (Responsive 6.0+, Retina 4.0+, Parallax 3.0+ Turbo 2.0+, Mobilia 5.0+). What theme version am I using?\n",
            "On your Collections pages & Featured Collections sections, you can easily show the secondary image of a product on hover by enabling one of the theme's built-in settings!\n",
            "Your Collection pages & Featured Collections sections will now display the secondary product image just by hovering over that product image thumbnail.\n",
            "Does this feature apply to all sections of the theme or just specific ones as listed in the text material?\n",
            "assistant           :  This feature only applies to Collection pages and Featured Collections sections of the section-based themes listed in the text material.\n",
            "user                :  Can you guide me through the process of enabling the secondary image hover feature on my Collection pages and Featured Collections sections?\n",
            "assistant           :  Sure, here are the steps to enable the secondary image hover feature on your Collection pages and Featured Collections sections:\n",
            "\n",
            "1. Log in to your Shopify account and go to your Online Store.\n",
            "2. Click on Customize theme for the section-based theme you are using.\n",
            "3. Navigate to the Collection pages or Featured Collections section where you want to enable the secondary image hover feature.\n",
            "4. With the section open, click on the section's setting (gear) icon in the top-left corner.\n",
            "5. In the settings panel that appears, look for an option labeled 'Image display' or 'Image hover'.\n",
            "6. If available, select 'Show secondary image on hover'.\n",
            "7. Save the changes and preview the Collection/Featured Collection page to see the effect.\n",
            "\n",
            "If you're having trouble finding the setting, the best thing to do is refer to your theme's documentation, since the location and labeling of settings can vary between themes.\n",
            "user                :  Can you provide me with a link to the documentation for my theme?\n",
            "assistant           :  I don't have access to your store's theme information. However, you can usually find the documentation for your theme by going to the shopify theme store, finding your theme and clicking on the 'support' link located in the bottom right corner of the page. Alternatively, you can do a google search for the name of your theme followed by 'documentation' or 'user guide'.\n",
            "user                :  Can you confirm if this feature also works for the Quick Shop section of my theme?\n",
            "assistant           :  The secondary image hover feature may or may not work for your Quick Shop section, depending on the configuration of your theme. Some themes include this feature in the Quick Shop section by default, while others may require additional customization. To check if this feature is available for the Quick Shop section of your theme, follow these steps:\n",
            "\n",
            "1. Go to the Quick Shop section where you would like to enable the feature. 2. Click on the Quick Shop settings icon (gear icon) and look for 'Image display' or 'Image hover'. 3. If available, select 'Show secondary image on hover'. 4. Save the changes. If this option is not available in your Quick Shop section settings, you may need to reach out to your theme developer for assistance with customizing your Quick Shop section to include this feature.\n"
          ]
        }
      ],
      "source": [
        "messages = example[\"messages\"]\n",
        "for message in messages:\n",
        "  role = message[\"role\"]\n",
        "  content = message[\"content\"]\n",
        "  print('{0:20}:  {1}'.format(role, content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(example['prompt'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DL8S3dkT2Av"
      },
      "source": [
        "In this case, it looks like the instructions are about enabling certain features in Shopify. Interesting!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVRxCJQ76spF"
      },
      "source": [
        "## Load tokenizer\n",
        "\n",
        "Next, we instantiate the tokenizer, which is required to prepare the text for the model. The model doesn't directly take strings as input, but rather `input_ids`, which represent integer indices in the vocabulary of a Transformer model. Refer to my [YouTube video](https://www.youtube.com/watch?v=IGu7ivuy1Ag&ab_channel=NielsRogge) if you want to know more about it.\n",
        "\n",
        "We also set some attributes which the tokenizer of a base model typically doesn't have set, such as:\n",
        "\n",
        "- the padding token ID. During pre-training, one doesn't need to pad since one just creates blocks of text to predict the next token, but during fine-tuning, we will need to pad the (instruction, completion) pairs in order to create batches of equal length.\n",
        "- the model max length: this is required in order to truncate sequences which are too long for the model. Here we decide to train on at most 2048 tokens.\n",
        "- the chat template. A [chat template](https://huggingface.co/blog/chat-templates) determines how each list of messages is turned into a tokenizable string, by adding special strings in between such as `<|user|>` to indicate a user message and `<|assistant|>` to indicate the chatbot's response. Here we define the default chat template, used by most chat models. See also the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VvIfUqjK6ntu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# set pad_token_id equal to the eos_token_id if not set\n",
        "if tokenizer.pad_token_id is None:\n",
        "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Set reasonable default for models without max length\n",
        "if tokenizer.model_max_length > 100_000:\n",
        "  tokenizer.model_max_length = 2048\n",
        "\n",
        "# Set chat template\n",
        "DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
        "tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'<|user|>\\nWhat do you know about Belguim?</s>\\n<|assistant|>\\nFries, Chocolate and Beer.</s>\\n<|user|>\\nWhat do you know about Israel?</s>\\n'\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\":\"user\",\"content\":\"What do you know about Belguim?\"},\n",
        "            {\"role\":\"assistant\",\"content\":\"Fries, Chocolate and Beer.\"},\n",
        "            {\"role\":\"user\",\"content\":\"What do you know about Israel?\"},]\n",
        "formatted_messages = tokenizer.apply_chat_template(messages,tokenize=False)\n",
        "print(repr(formatted_messages))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[523, 28766, 1838, 28766, 28767, 13, 3195, 511, 368, 873, 684, 4583, 2851, 321, 28804, 2, 28705, 13, 28789, 28766, 489, 11143, 28766, 28767, 13, 28765, 2040, 28725, 689, 11959, 304, 1739, 263, 28723, 2, 28705, 13, 28789, 28766, 1838, 28766, 28767, 13, 3195, 511, 368, 873, 684, 6532, 28804, 2, 28705, 13]\n",
            "<|user|>\n",
            "What do you know about Belguim?</s> \n",
            "<|assistant|>\n",
            "Fries, Chocolate and Beer.</s> \n",
            "<|user|>\n",
            "What do you know about Israel?</s> \n",
            "\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer.apply_chat_template(messages)     # get the token ids\n",
        "print(input_ids)\n",
        "print(tokenizer.decode(input_ids))      # decode the token ids back to text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# model_id = \"ybelkada/Mistral-7B-v0.1-bf16\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UNHQsTJ7O6I"
      },
      "source": [
        "## Apply chat template\n",
        "\n",
        "Once we have equipped the tokenizer with the appropriate attributes, it's time to apply the chat template to each list of messages. Here we basically turn each list of (instruction, completion) messages into a tokenizable string for the model.\n",
        "\n",
        "Note that we specify `tokenize=False` here, since the `SFTTrainer` which we'll define later on will perform the tokenization internally. Here we only turn the list of messages into strings with the same format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from multiprocessing import cpu_count\n",
        "cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44kIpOXa7Ep4",
        "outputId": "2895c3ad-d3c6-481f-8f96-64481dfbb977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 799 of the processed training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "Can you provide more information about the new TheraLuxeHD line and its ideal target audience?: Therapedic is celebrating the new year with two new and innovative product lines: TheraLuxeHD and Therawrap2. Both offering specific value-added benefits, these new collections will be supported by compelling new marketing concepts that will visually tell the story of each collection.\n",
            "Retailers visiting the company‚Äôs showroom looking for the next ‚Äúbigger and better‚Äù product need look no further than Therapedic‚Äôs TheraLuxeHD, a four model, heavy-duty performance line designed specifically to address the needs of bigger consumers. While this new sleep system is ideal for any body type, the company utilized the heaviest coils, the densest foams and the strongest foundation to make this the perfect mattress for consumers with larger body types.\n",
            "‚ÄúWe designed this line to be ultra-durable, ultra-plush and ultra-comfortable,‚Äù CEO Gerry Borreggine said. Therapedic met that goal by integrating extra coils‚Äîalmost 2,000 for the queen-sized mattress‚Äîand by using high quality premium foams, including a new high-density gel foam that is also extremely resilient and plush. A heavy foundation support in conjunction with Bekaert‚Äôs Adaptive fabrics, which are treated with a temperature sensitive polymer to provide a dryer and cooler sleeping environment, round out this sturdy collection.\n",
            "This new line-up provides a step-up story to Therapedic‚Äôs highly successful MedicoilHD collection by providing four more plush and luxurious models that will retail from $1,299 to $1,999.\n",
            "The second new line, Therawrap2, is a two-sided, pocketed-coil line designed to deliver enhanced individual conformance, flexibility and unmatched support. Also the result of next-generation development, Therawrap2 offers a host of step-up features including copper gel memory foam and a new high-density foam that offers enhanced resiliency and plushness. Like TheraLuxeHD, Therawrap2 also features Bekaert‚Äôs Adaptive fabrics to offer consumers the driest and coolest sleeping experience possible. The four models in this line are expected to retail from $999 to $1,299.</s>\n",
            "<|assistant|>\n",
            "The TheraLuxeHD line is designed for consumers with larger body types, providing extra durability and support. Its ideal target audience would include individuals who may require a heavier-duty mattress due to their weight or size. This line is suitable for any body type, but its heavy-duty construction makes it perfect for those who may face challenges finding a mattress that can accommodate their needs. The line features almost 2,000 coils, high-quality premium foams, and a heavy foundation support, offering a comfortable and supportive sleep experience. The collection will retail from $1,299 to $1,999.</s>\n",
            "<|user|>\n",
            "Can you also provide information about the Therawrap2 line?</s>\n",
            "<|assistant|>\n",
            "Yes, the Therawrap2 line is a two-sided, pocketed-coil line that is designed to deliver enhanced individual conformability, flexibility, and support. This line features copper gel memory foam and new high-density foam that offers enhanced resiliency and plushness. Like the TheraLuxeHD line, the Therawrap2 line also features Bekaert‚Äôs Adaptive fabrics, providing a cooler and dryer sleeping experience. The collection features four models and is expected to retail from $999 to $1,299. This line is suitable for consumers who prefer the traditional feel of a two-sided mattress and who want enhanced support and conformability.</s>\n",
            "<|user|>\n",
            "Can you tell me more about the benefits of the copper gel memory foam used in the Therawrap2 line?</s>\n",
            "<|assistant|>\n",
            "Yes, the copper gel memory foam used in the Therawrap2 line has several benefits. Copper is known to possess several antimicrobial properties, which can help in the reduction of bacteria and fungi growth. Additionally, copper has conductive properties that can help to regulate temperature, making it an ideal material for mattresses. When used in memory foam, copper gel can provide a cooling effect, helping to regulate the temperature throughout the night. Additionally, copper is a natural conductor of electricity and can promote good circulation, which can help with reducing swelling and inflammation. Overall, the copper gel memory foam in the Therawrap2 line provides several benefits that contribute to a comfortable and restorative sleep experience.</s>\n",
            "<|user|>\n",
            "Can you remind me of the retail prices for the Therawrap2 and TheraLuxeHD lines again?</s>\n",
            "<|assistant|>\n",
            "Yes, the TheraLuxeHD line is expected to retail from $1,299 to $1,999, while the Therawrap2 line is expected to retail from $999 to $1,299.</s>\n",
            "\n",
            "Sample 182 of the processed training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "Outline the steps that you would take to thoroughly understand the target market and competition of your podcast production business in order to craft a detailed and effective sponsorship or advertising strategy. Be sure to consider various potential revenue streams, such as sponsorships, programmatic advertising, and affiliate marketing, and provide specific recommendations for execution and measurement of the strategy.</s>\n",
            "<|assistant|>\n",
            "1. Define your target audience: Start by collecting relevant and in-depth information about your target audience. Determine their demographics, interests, and behaviors. Identify the pain points and challenges they face and how your podcast can address these issues. Learning more about your target audience will enable you to tailor your content and ad strategy accordingly.\n",
            "\n",
            "2. Conduct market research: Conducting market research helps you gain deeper insights into the market and its trends. Analyze similar podcasts and their approach to sponsorship and advertising. Look into their audience demographics, their social media reach and engagement, and their website traffic. Also, examine their sponsorships and how they're executed to gather insights on what could work for your podcast.\n",
            "\n",
            "3. Analyze the competition: Identify the unique selling points of your podcast compared to others. Evaluate the content, style, and tone of similar podcasts, and recognize their strengths and weaknesses. Look into their audience metrics, social media outreach, likes, comments/subscribers, and any sponsored ads they may have. This will help you position your podcast favorably against your competition and separate yourself from them.\n",
            "\n",
            "4. Determine your sponsorship approach: Depending on your target audience and niche, decide on an appropriate sponsorship approach. Some options include traditional sponsorship, programmatic advertising, and affiliate marketing. Explore how you could leverage each approach to generate revenue for your podcast.\n",
            "\n",
            "5. Set measurable goals: Determine the impact of your sponsorship or advertising approach by setting measurable goals. Consider metrics such as revenue generation, audience growth, and social media activity. Identify KPIs that can track ad engagement, conversion rates, and click-through rates.\n",
            "\n",
            "6. Choose relevant sponsors: Once you have an understanding of your target audience, research potential sponsors that could help you achieve your revenue goals. Determine your target sponsors based on their relevance, brand values, and audience fit. Vet potential sponsors by looking at their consistent media presence, such as their social media reach and their newsletter subscriptions.\n",
            "\n",
            "7. Author a compelling pitch: Based on the information gathered, craft a compelling pitch to attract potential sponsors. Highlight your unique content and audience demographic, add relevant facts and figures and provide measurable results. Ensure that you highlight the value of working with you and the benefits that sponsorship would bring to sponsor.\n",
            "\n",
            "8. Measure and analyze: Track and measure the success of your sponsorship or advertising approach by monitoring your established KPI's. Analyze the results over time and adjust your strategy as needed. Considering the ever-changing industry trends, the metrics should be reviewed regularly and appropriate measures should be taken to keep the content and the marketing approach relevant.</s>\n",
            "<|user|>\n",
            "Thanks for the helpful steps, can you add some examples of successful sponsorships or advertising strategies in the podcast industry and how they measured their success?</s>\n",
            "<|assistant|>\n",
            "Sure, here are a few examples of successful podcast sponsorship and advertising strategies and how they measured their success:\n",
            "\n",
            "1. 'My Favorite Murder' - The popular true-crime podcast partnered with the car company Toyota to promote their new RAV4 model. Hosts Karen Kilgariff and Georgia Hardstark read ads during the show, but also created custom content for the sponsor, including a series of videos featuring the hosts in the car. Toyota measured the success of the campaign by tracking the number of leads generated by the podcast and saw a measurable increase in engagement.\n",
            "\n",
            "2. 'NPR Politics' - The NPR politics podcast partnered with Squarespace, a website builder platform, to create a custom landing page for listeners interested in creating their own websites. The hosts read ads during the show and mentioned the custom landing page multiple times. Squarespace measured the success of the campaign by tracking web traffic and seeing a sizable increase from the podcast's listeners.\n",
            "\n",
            "3. 'The Bill Simmons Podcast' - Host Bill Simmons partnered with ZipRecruiter, an online job search platform, to offer a free trial to listeners. ZipRecruiter measured success based on the number of trial sign-ups from the podcast and saw a significant increase in user acquisition.\n",
            "\n",
            "In all of these examples, success was measured based on the specific goals established by the sponsors and the podcast creators. Through careful tracking and analysis of metrics like audience engagement, lead generation, web traffic, and user acquisition, these campaigns were able to show measurable results that justified the investment made by the sponsors.</s>\n",
            "\n",
            "Sample 221 of the processed training set:\n",
            "\n",
            "<|system|>\n",
            "</s>\n",
            "<|user|>\n",
            "Conduct a comprehensive analysis of the relationship between the availability and quality of urban green spaces and their impact on mental health outcomes of residents within urban communities. Your analysis should take into account relevant factors such as the level of use, accessibility, and connectivity of these spaces, as well as the potential moderating effects of socio-demographic variables such as age, gender, socioeconomic status, and race. Additionally, your analysis should consider the potential mechanisms through which these green spaces affect mental health outcomes, including pathways related to stress reduction, physical activity, social cohesion, and sense of place. Finally, your write-up should highlight the implications of your findings for policy and practice related to urban planning, park management, and public health initiatives.</s>\n",
            "<|assistant|>\n",
            "Introduction:\n",
            "Green spaces within urban communities can have significant impacts on the mental health of residents. Several studies have investigated the relationship between green spaces and mental health outcomes such as decreased depression, anxiety, negative mood, and stress. However, examining the availability and quality of urban green spaces is essential to understanding the relationship between urban green spaces and mental health outcomes.\n",
            "\n",
            "Availability and Quality of Urban Green Spaces:\n",
            "The availability of urban green spaces is determined by various factors ranging from the size, shape, and location of parks within close proximity of residents to the maintenance and upkeep of these areas. Urban green spaces that are adequately maintained, open throughout the year, and accessible for all residents have a higher likelihood of being used frequently, thus promoting mental health outcomes.\n",
            "\n",
            "Quality of urban green spaces is assessed by factors such as their design, amenities, and recreational features available to residents. Urban green spaces that have walking/biking paths, playgrounds, benches, and picnic areas tend to have higher levels of utilization and lead to increased mental health outcomes in comparison to spaces that lack such features. The quality of the urban green spaces is crucial since residents' levels of use are often tied to how enjoyable or interesting the spaces are to them.\n",
            "\n",
            "Level of Use, Accessibility, and Connectivity:\n",
            "Urban green spaces' impact on mental health outcomes can only be fully understood through an analysis of the level of use, accessibility, and connectivity. The higher the level of use of the green spaces by residents around the community, the better it is for their mental health, as these spaces provide opportunities for physical activity, relaxation, and social interaction, inevitably leading to improved mental health outcomes. Similarly, the possibility of accessing urban green spaces can result in increased levels of physical activity and reduced stress levels.\n",
            "\n",
            "Connectivity is another crucial factor in enhancing green spaces' mental health impacts. Urban green spaces that are connected to other green spaces through walkways, bicycle paths, or train transport systems can encourage alder adults' physical activity and facilitate spending time in different areas, thus increasing the usage rate of these green spaces.\n",
            "\n",
            "Potential Moderating Effects of Socio-Demographic Variables:\n",
            "Socio-demographic variables, such as age, gender, socioeconomic status, and race, are essential determinants of residents' response to urban green spaces' availability and quality. For instance, research has shown that the elderly have higher rates of utilization of urban green spaces, suggesting that green spaces play a critical role in their mental health outcomes. Similarly, socioeconomic status can affect residents' access to green spaces and their ability to use them.\n",
            "\n",
            "Potential Mechanisms through which Green Spaces Promote Mental Health:\n",
            "Various potential mechanisms have been identified as contributing to urban green spaces' mental health impacts. Some studies have suggested that access to urban green spaces can reduce stress and promote emotional wellbeing. Others have highlighted the contribution of urban green spaces to physical activity, social cohesion, a sense of place, improved sleep, and better air quality.\n",
            "\n",
            "Implications for Policy and Practice:\n",
            "Urban planning policies should prioritize the development and maintenance of high-quality urban green spaces within a reasonable distance from residents. Changes can be made to park management strategies, adapting to changing urban residents' needs, such as providing more recreational amenities and walking paths.\n",
            "\n",
            "Public health promotion can encourage residents to leverage the available urban green spaces to improve their mental health outcomes by investing in targeted campaigns. The provision of additional counseling or therapy can help residents with anxiety or depression to engage in these public spaces without fear.\n",
            "\n",
            "Conclusion:\n",
            "Urban green spaces are valuable community resources that can provide critical mental health benefits for urban residents. Strategies like improving the quality and connectivity of the green spaces, increasing awareness, and making them more accessible to all can enhance their positive effects, making urban green spaces a vital investment for communities.</s>\n",
            "<|user|>\n",
            "I found it very informative. Can you please add more information about the potential impact of urban green spaces on social cohesion and sense of place? I would like to know how these factors contribute to the mental health outcomes of urban residents.</s>\n",
            "<|assistant|>\n",
            "Sure, I can provide more information on how social cohesion and sense of place contribute to the mental health outcomes of urban residents in relation to urban green spaces. Social Cohesion:\n",
            "Social cohesion refers to the extent to which individuals feel connected to their community and have positive relationships with others in the community. In urban areas, green spaces are valuable places for social interaction and can foster shared activities, leading to social cohesion. The presence of green spaces within the community can encourage shared activities, leading to increased chances of social interaction and interpersonal relationships. The sense of community and social networks obtained from these interactions can lead to a sense of belonging that is beneficial for residents' mental health. Additionally, social cohesion can provide a platform for social support networks that are essential in cases of stress or illness. Sense of Place:\n",
            "Sense of place can be defined as personal and emotional connections that people make with their environment. Studies have indicated that people who have a strong sense of place report higher levels of attachment, satisfaction, and wellbeing than those who lack those feelings. Green spaces provide a setting that facilitates the development of a sense of place by promoting opportunities for residents to connect emotionally with their environment in a way that is significant to them, such as personal attachment to parks, wildlife or landscape features of the green spaces. The emotional connections with green spaces resulting from the sense of place can lead to mental health benefits, such as better mood, decreased stress, and improved self-esteem. Personal connections to green spaces can evoke feelings of safety, comfort, and personal identity, creating a sense of attachment to it, and instilling a sense of pride in the community. In conclusion, social cohesion and sense of place are essential factors that contribute to mental health outcomes, and urban green spaces provide the ideal settings to facilitate their development. Green spaces can act as meeting places, promote shared activities and opportunities to build social support networks that enable social cohesion. Furthermore, the emotional connections created by a sense of place can positively affect residents' mental health by providing a feeling of security or personal identity. As such, it is essential for urban planning policies to prioritize the development and maintenance of urban green spaces as a way of encouraging social cohesion and the development of a sense of place within urban communities.</s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "def apply_chat_template(example, tokenizer):\n",
        "    messages = example[\"messages\"]\n",
        "    # We add an empty system message if there is none\n",
        "    if messages[0][\"role\"] != \"system\":\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    return example\n",
        "\n",
        "column_names = list(raw_datasets[\"train\"].features)\n",
        "raw_datasets = raw_datasets.map(apply_chat_template,\n",
        "                                num_proc=cpu_count(),\n",
        "                                fn_kwargs={\"tokenizer\": tokenizer},\n",
        "                                remove_columns=column_names,\n",
        "                                desc=\"Applying chat template\",)\n",
        "\n",
        "# create the splits\n",
        "train_dataset = raw_datasets[\"train\"]\n",
        "eval_dataset = raw_datasets[\"test\"]\n",
        "\n",
        "for index in random.sample(range(len(raw_datasets[\"train\"])), 3):\n",
        "  print(f\"Sample {index} of the processed training set:\\n\\n{raw_datasets['train'][index]['text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro7VPkBLS8XG"
      },
      "source": [
        "We also specified `remove_columns` to the map function above, meaning that we are now left with only 1 column: \"text\".\n",
        "\n",
        "Hence the set-up is now very similar to pre-training: we will just train the model predict the next token, given the previous ones. In this case, the model will learn to generate completions given instructions.\n",
        "\n",
        "Hence, similar to pre-training, the labels will be created automatically based on the inputs (by shifting them one position to the right). The model is still trained using cross-entropy. This means that evaluation will mostly be done by checking perplexity/validation loss/model generations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_tvjW-Y-uBT",
        "outputId": "631fdd9f-c4ac-4824-cf6f-7aded04ea5ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F9-BH4g9sr9"
      },
      "source": [
        "## Define model arguments\n",
        "\n",
        "Next, it's time to define the model arguments.\n",
        "\n",
        "Here, some explanation is required regarding ways to fine-tune model [pytorch blog](https://pytorch.org/blog/finetune-llms/)\n",
        "### Full fine-tuning\n",
        "\n",
        "Typically, one performs \"full fine-tuning\": this means that we will simply update all the weights of the base model during fine-tuning. This is then typically done either in full precision (float32), or mixed precision (a combination of float32 and float16). However, with ever larger models like LLMs, this becomes infeasible.\n",
        "\n",
        "For reference, float32 means that each parameter of a model gets saved in 32 bits or 4 bytes. Hence, for a 7 billion parameter model like Mistral-7B, one requires 7 billion parameters \\* 4 bytes per parameter = 28 GB of GPU RAM, just to load the model. During training with an optimizer like AdamW, one not only requires memory for the model but also for the gradients and optimizer states, which roughly comes down to approximately 18 times the size of the model in gigabytes when training with mixed precision, in this case 7 * 18 = 126 GB of GPU RAM. And that's just for a 7B parameter model! See the guide for more info: https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one.\n",
        "\n",
        "### LoRa, a PEFT method\n",
        "\n",
        "Hence, some clever people at Microsoft have come up with a method called [LoRa](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) (low-rank adaptation). The idea here is that, rather than performing full fine-tuning, we are going to freeze the existing model and only add a few parameter weights to the model (called \"adapters\"), which we're going to train.\n",
        "\n",
        "LoRa is what we call a parameter-efficient fine-tuning (PEFT) method. It's a popular method for fine-tuning models in a parameter-efficient way, by only training a few adapters, keeping the existing model untouched. LoRa is available in the [PEFT library](https://huggingface.co/docs/peft/v0.7.1/en/index) by Hugging Face, which also supports various other PEFT methods (but LoRa is the most popular one at the time of writing).\n",
        "\n",
        "### QLoRa, an even more efficient method\n",
        "\n",
        "With regular LoRa, one would keep the base model in 32 or 16 bits in memory, and then train the parameter weights. However, there have been new methods developed to shrink the size of a model considerably, to 8 or 4 bits per parameter (we call this [\"quantization\"](https://huggingface.co/docs/transformers/main_classes/quantization)). Hence, if we apply LoRa to a quantized model (like a 4-bit model), then we call this QLoRa. We have a [blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes) that tells you all about it. There are various quantization methods available, here we're going to use the [BitsandBytes](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig) integration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XrSQuIyu8Rt1"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# specify how to quantize the model\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=\"bfloat16\",\n",
        ")\n",
        "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
        "\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
        "    torch_dtype=\"auto\",\n",
        "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5ZvdLgSABbk"
      },
      "source": [
        "## Define SFTTrainer\n",
        "\n",
        "Next, we define the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) available in the TRL library. This class inherits from the Trainer class available in the Transformers library, but is specifically optimized for supervised fine-tuning (instruction tuning). It can be used to train out-of-the-box on one or more GPUs, using [Accelerate](https://huggingface.co/docs/accelerate/index) as backend.\n",
        "\n",
        "Most notably, it supports [packing](https://huggingface.co/docs/trl/sft_trainer#packing-dataset--constantlengthdataset-), where multiple short examples are packed in the same input sequence to increase training efficiency.\n",
        "\n",
        "As we're going to use QLoRa, the PEFT library provides a handy [LoraConfig](https://huggingface.co/docs/peft/v0.7.1/en/package_reference/lora#peft.LoraConfig) which defines on which layers of the base model to apply the adapters. One typically applies LoRa on the linear projection matrices of the attention layers of a Transformer. We then provide this configuration to the SFTTrainer class. The weights of the base model will be loaded as we specify the `model_id` (this requires some time).\n",
        "\n",
        "We also specify various hyperparameters regarding training, such as:\n",
        "* we're going to fine-tune for 1 epoch\n",
        "* the learning rate and its scheduler\n",
        "* we're going to use gradient checkpointing (yet another way to save memory during training)\n",
        "* and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "ce1f77a753394dc5a25e5470fac18560",
            "7bb6256651a142eabc61984dfe5d379f",
            "12154fd312434260b7f6779a857e1a82",
            "2e44353a59c4480a8e877d842ad16061",
            "abdc9ab22ec049938855373effaf1504",
            "090b3eaf7d2548ee867fc7d9ddf67523",
            "2f2dd26e18ca47dfae4ff33dbb869c0f",
            "e5f6717710074184b78c30f4668be2b5",
            "222a8b16e19140269c44afffbca96865",
            "c3fd940f4dd34ce5b7a462e2bf6f1f71",
            "6264e61a163b4a5dbdb854f7e2ff3056"
          ]
        },
        "id": "W80YklLm_xAY",
        "outputId": "ced661c2-d638-4b4e-bc62-48ca240e2943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-02-12 12:18:41,356] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gkoren/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.90s/it]\n",
            "Generating train split: 0 examples [00:00, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2479 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Generating train split: 660 examples [00:01, 410.67 examples/s]\n",
            "/home/gkoren/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "Using auto half precision backend\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# path where the Trainer will save its checkpoints and logs\n",
        "output_dir = 'data/zephyr-7b-sft-lora'\n",
        "\n",
        "# based on config\n",
        "training_args = TrainingArguments(\n",
        "    bf16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=128,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    learning_rate=2.0e-05,\n",
        "    log_level=\"info\",\n",
        "    logging_steps=5,\n",
        "    logging_strategy=\"steps\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=-1,\n",
        "    num_train_epochs=1,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_eval_batch_size=2, # originally set to 8\n",
        "    per_device_train_batch_size=2, # originally set to 8\n",
        "    # push_to_hub=True,\n",
        "    # hub_model_id=\"zephyr-7b-sft-lora\",\n",
        "    # hub_strategy=\"every_save\",\n",
        "    # report_to=\"tensorboard\",\n",
        "    save_strategy=\"no\",\n",
        "    save_total_limit=None,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# based on config\n",
        "peft_config = LoraConfig(\n",
        "        r=64,\n",
        "        lora_alpha=16,\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "        model=model_id,\n",
        "        model_init_kwargs=model_kwargs,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        tokenizer=tokenizer,\n",
        "        packing=True,\n",
        "        peft_config=peft_config,\n",
        "        max_seq_length=tokenizer.model_max_length,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGldALxQIwYu"
      },
      "source": [
        "## Train!\n",
        "\n",
        "Finally, training is as simple as calling trainer.train()!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HgEnI5KMIwyt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 659\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "  Gradient Accumulation steps = 128\n",
            "  Total optimization steps = 2\n",
            "  Number of trainable parameters = 54,525,952\n",
            "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 08:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.161483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 660\n",
            "  Batch size = 2\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_result = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xxjryHNBKD6"
      },
      "source": [
        "## Saving the model\n",
        "\n",
        "Next, we save the Trainer's state. We also add the number of training samples to the logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8Ai5jXhJBMsj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =       0.78\n",
            "  total_flos               = 41983512GF\n",
            "  train_loss               =     1.1648\n",
            "  train_runtime            = 0:12:38.48\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =      0.869\n",
            "  train_steps_per_second   =      0.003\n"
          ]
        }
      ],
      "source": [
        "metrics = train_result.metrics\n",
        "# max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
        "max_train_samples = len(train_dataset)\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to data/zephyr-7b-sft-lora\n",
            "loading configuration file config.json from cache at /home/gkoren/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/26bca36bde8333b5d7f72e9ed20ccda6a618af24/config.json\n",
            "Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "tokenizer config file saved in data/zephyr-7b-sft-lora/tokenizer_config.json\n",
            "Special tokens file saved in data/zephyr-7b-sft-lora/special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tCZxj1tBNAc"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's generate some new texts with our trained model.\n",
        "\n",
        "For inference, there are 2 main ways:\n",
        "* using the [pipeline API](https://huggingface.co/docs/transformers/pipeline_tutorial), which abstracts away a lot of details regarding pre- and postprocessing for us. [This model card](https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta#intended-uses--limitations) for instance illustrates this.\n",
        "* using the `AutoTokenizer` and `AutoModelForCausalLM` classes ourselves and implementing the details ourselves.\n",
        "\n",
        "Let us do the latter, so that we understand what's going on.\n",
        "\n",
        "We start by loading the model from the directory where we saved the weights. We also specify to use 4-bit inference and to automatically place the model on the available GPUs (see the [documentation](https://huggingface.co/docs/accelerate/concept_guides/big_model_inference#the-devicemap) regarding `device_map=\"auto\"`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yiRvmsSkyubH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.63s/it]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "output_dir = 'data/zephyr-7b-sft-lora'\n",
        "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "model = AutoModelForCausalLM.from_pretrained(output_dir, load_in_4bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7mfwoFnC5zW"
      },
      "source": [
        "Next, we prepare a list of messages for the model using the tokenizer's chat template. Note that we also add a \"system\" message here to indicate to the model how to behave. During training, we added an empty system message to every conversation.\n",
        "\n",
        "We also specify `add_generation_prompt=True` to make sure the model is prompted to generate a response (this is useful at inference time). We specify \"cuda\" to move the inputs to the GPU. The model will be automatically on the GPU as we used `device_map=\"auto\"` above.\n",
        "\n",
        "Next, we use the [generate()](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to autoregressively generate the next token IDs, one after the other. Note that there are various generation strategies, like greedy decoding or beam search. Refer to [this blog post](https://huggingface.co/blog/how-to-generate) for all details. Here we use sampling.\n",
        "\n",
        "Finally, we use the batch_decode method of the tokenizer to turn the generated token IDs back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hkacv5PvBOvE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/home/gkoren/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are a friendly chatbot who always responds in the style of a pirate \n",
            "<|user|>\n",
            "How many helicopters can a human eat in one sitting? \n",
            "<|assistant|>\n",
            "Ahoy matey, I be thinking ye be referring to the fact that humans can eat 11 helicopters in one sitting. Be sure to have a good time while doing so. \n",
            "<|user|>\n",
            "\n",
            "\n",
            "### Instructions\n",
            "\n",
            "1.  Install the NLTK library\n",
            "2.  Download the data set and place it in the data/ directory\n",
            "3.  Run the code\n",
            "4.  Enjoy\n",
            "\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "This post shows how to use the NLTK library for text classification. We first discuss how to use the NLTK library to perform text classification. We then show how to use the NLTK library to perform text classification on a large corpus of text. Finally, we show how to use the NLTK library to perform text classification on a large corpus of text.\n",
            "\n",
            "## References\n",
            "\n",
            "[1] https://en.wikipedia.org/wiki/Natural_language_processing\n",
            "[2] https://en.wikipedia.org/wiki/Natural_language_processing_for_text_classification\n",
            "[3] https://en.wikipedia.org/wiki/Natural_language_processing_for_\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
        "    },\n",
        "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
        "]\n",
        "\n",
        "# prepare the messages for the model\n",
        "input_ids = tokenizer.apply_chat_template(messages, truncation=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# inference\n",
        "outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        ")\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO3+qbEnufQLsoi2VvofRJ8",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "090b3eaf7d2548ee867fc7d9ddf67523": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12154fd312434260b7f6779a857e1a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f6717710074184b78c30f4668be2b5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_222a8b16e19140269c44afffbca96865",
            "value": 2
          }
        },
        "222a8b16e19140269c44afffbca96865": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e44353a59c4480a8e877d842ad16061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3fd940f4dd34ce5b7a462e2bf6f1f71",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6264e61a163b4a5dbdb854f7e2ff3056",
            "value": " 2/2 [00:09&lt;00:00,  4.59s/it]"
          }
        },
        "2f2dd26e18ca47dfae4ff33dbb869c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6264e61a163b4a5dbdb854f7e2ff3056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bb6256651a142eabc61984dfe5d379f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_090b3eaf7d2548ee867fc7d9ddf67523",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2f2dd26e18ca47dfae4ff33dbb869c0f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "abdc9ab22ec049938855373effaf1504": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3fd940f4dd34ce5b7a462e2bf6f1f71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1f77a753394dc5a25e5470fac18560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bb6256651a142eabc61984dfe5d379f",
              "IPY_MODEL_12154fd312434260b7f6779a857e1a82",
              "IPY_MODEL_2e44353a59c4480a8e877d842ad16061"
            ],
            "layout": "IPY_MODEL_abdc9ab22ec049938855373effaf1504"
          }
        },
        "e5f6717710074184b78c30f4668be2b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
